{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c990117-f2af-4f09-8576-715a6a99a22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "\n",
    "BASE_MODEL = \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "\n",
    "MIN_TOKENS = 150 # Any less than this, and we don't have enough useful content\n",
    "MAX_TOKENS = 160 # Truncate after this many tokens. Then after adding in prompt text, we will get to around 180 tokens\n",
    "\n",
    "MIN_CHARS = 300\n",
    "CEILING_CHARS = MAX_TOKENS * 7\n",
    "\n",
    "class Item:\n",
    "    \"\"\"\n",
    "    An Item is a cleaned, curated datapoint of a Product with a Price\n",
    "    \"\"\"\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "    PREFIX = \"Price is $\"\n",
    "    QUESTION = \"How much does this cost to the nearest dollar?\"\n",
    "    REMOVALS = ['\"Batteries Included?\": \"No\"', '\"Batteries Included?\": \"Yes\"', '\"Batteries Required?\": \"No\"', '\"Batteries Required?\": \"Yes\"', \"By Manufacturer\", \"Item\", \"Date First\", \"Package\", \":\", \"Number of\", \"Best Sellers\", \"Number\", \"Product \"]\n",
    "\n",
    "    title: str\n",
    "    price: float\n",
    "    category: str\n",
    "    token_count: int = 0\n",
    "    details: Optional[str]\n",
    "    prompt: Optional[str] = None\n",
    "    include = False\n",
    "\n",
    "    def __init__(self, data, price):\n",
    "        self.title = data['title']\n",
    "        self.price = price\n",
    "        self.parse(data)\n",
    "\n",
    "    def scrub_details(self):\n",
    "        \"\"\"\n",
    "        Clean up the details string by removing common text that doesn't add value\n",
    "        \"\"\"\n",
    "        details = self.details\n",
    "        for remove in self.REMOVALS:\n",
    "            details = details.replace(remove, \"\")\n",
    "        return details\n",
    "\n",
    "    def scrub(self, stuff):\n",
    "        \"\"\"\n",
    "        Clean up the provided text by removing unnecessary characters and whitespace\n",
    "        Also remove words that are 7+ chars and contain numbers, as these are likely irrelevant product numbers\n",
    "        \"\"\"\n",
    "        stuff = re.sub(r'[:\\[\\]\"{}【】\\s]+', ' ', stuff).strip()\n",
    "        stuff = stuff.replace(\" ,\", \",\").replace(\",,,\",\",\").replace(\",,\",\",\")\n",
    "        words = stuff.split(' ')\n",
    "        select = [word for word in words if len(word)<7 or not any(char.isdigit() for char in word)]\n",
    "        return \" \".join(select)\n",
    "    \n",
    "    def parse(self, data):\n",
    "        \"\"\"\n",
    "        Parse this datapoint and if it fits within the allowed Token range,\n",
    "        then set include to True\n",
    "        \"\"\"\n",
    "        contents = '\\n'.join(data['description'])\n",
    "        if contents:\n",
    "            contents += '\\n'\n",
    "        features = '\\n'.join(data['features'])\n",
    "        if features:\n",
    "            contents += features + '\\n'\n",
    "        self.details = data['details']\n",
    "        if self.details:\n",
    "            contents += self.scrub_details() + '\\n'\n",
    "        if len(contents) > MIN_CHARS:\n",
    "            contents = contents[:CEILING_CHARS]\n",
    "            text = f\"{self.scrub(self.title)}\\n{self.scrub(contents)}\"\n",
    "            tokens = self.tokenizer.encode(text, add_special_tokens=False)\n",
    "            if len(tokens) > MIN_TOKENS:\n",
    "                tokens = tokens[:MAX_TOKENS]\n",
    "                text = self.tokenizer.decode(tokens)\n",
    "                self.make_prompt(text)\n",
    "                self.include = True\n",
    "\n",
    "    def make_prompt(self, text):\n",
    "        \"\"\"\n",
    "        Set the prompt instance variable to be a prompt appropriate for training\n",
    "        \"\"\"\n",
    "        self.prompt = f\"{self.QUESTION}\\n\\n{text}\\n\\n\"\n",
    "        self.prompt += f\"{self.PREFIX}{str(round(self.price))}.00\"\n",
    "        self.token_count = len(self.tokenizer.encode(self.prompt, add_special_tokens=False))\n",
    "\n",
    "    def test_prompt(self):\n",
    "        \"\"\"\n",
    "        Return a prompt suitable for testing, with the actual price removed\n",
    "        \"\"\"\n",
    "        return self.prompt.split(self.PREFIX)[0] + self.PREFIX\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Return a String version of this Item\n",
    "        \"\"\"\n",
    "        return f\"<{self.title} = ${self.price}>\"\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691d4e59-9e25-4df6-a82a-9046cfa3efd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db63361-4452-442d-8856-298d873c0217",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
